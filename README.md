# GPU Scheduler

A complete Kubernetes GPU scheduling system with custom scheduler and mutating admission webhook that assigns pods to specific nodes and automatically injects CUDA_VISIBLE_DEVICES environment variables.

## Overview

This project provides a two-component solution: a custom Kubernetes scheduler for pod placement and a mutating admission webhook for automatic CUDA_VISIBLE_DEVICES environment variable injection. Together, they process GPU scheduling annotations and assign pods to specified nodes with the correct GPU device visibility. The system includes a complete GitOps deployment pipeline using ArgoCD and comprehensive testing capabilities.

## Features

- **Custom GPU Scheduler**: Kubernetes scheduler that interprets GPU assignment annotations
- **Mutating Admission Webhook**: Automatically injects CUDA_VISIBLE_DEVICES environment variables
- **Pod Placement**: Assigns pods to specific nodes based on configuration
- **Environment Variable Injection**: Sets `CUDA_VISIBLE_DEVICES` for GPU device visibility before pod creation
- **TLS Security**: Secure webhook communication with certificate management
- **Test Service**: Validates GPU scheduling functionality with continuous logging
- **GitOps Deployment**: ArgoCD ApplicationSets for automated deployment
- **Production Ready**: Helm charts with security best practices
- **Local Development**: KinD cluster setup for testing

## Quick Start

ğŸš€ **NEW: Complete Setup Guide Available!**  
For a comprehensive step-by-step guide from scratch to working system, see: **[COMPLETE_SETUP_GUIDE.md](COMPLETE_SETUP_GUIDE.md)**

### Prerequisites

- Kubernetes cluster (or KinD for local development)
- Helm 3.x
- ArgoCD (optional, for GitOps deployment)
- Docker (for building images)

## Architecture

```
User Creates Pod
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Mutating Admission  â”‚    â”‚  Custom GPU         â”‚
â”‚     Webhook         â”‚â”€â”€â”€â–¶â”‚   Scheduler         â”‚
â”‚ (Inject CUDA_VARS)  â”‚    â”‚ (Pod Placement)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                              â”‚
      â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pod with GPU Env   â”‚    â”‚  Pod on Target      â”‚
â”‚     Variables       â”‚    â”‚      Node           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Deployment Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ArgoCD        â”‚    â”‚  GPU Scheduler   â”‚    â”‚ Test Service    â”‚
â”‚  ApplicationSet â”‚â”€â”€â”€â–¶â”‚     Helm Chart   â”‚â”€â”€â”€â–¶â”‚   Helm Chart    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                       â”‚
         â”‚                        â–¼                       â–¼
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚Scheduler+Webhook â”‚    â”‚  Test Pods      â”‚
         â”‚              â”‚  (Control Plane) â”‚    â”‚ (Worker Nodes)  â”‚
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Kubernetes API     â”‚
                        â”‚   Pod Scheduling     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Usage

### GPU Scheduling Annotation

Add the following annotation to your pod or deployment:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-workload-0
  annotations:
    gpu-scheduling-map: |
      0=node1:0,1
      1=node2:2
      2=node3:0,1,2
      3=node4:3
      4=node4:3
spec:
  schedulerName: gpu-scheduler
  containers:
  - name: gpu-app
    image: nvidia/cuda:11.0-base
    command: ["nvidia-smi"]
    # CUDA_VISIBLE_DEVICES automatically injected!
```

### Annotation Format

```
gpu-scheduling-map: |
  <pod_index>=<node_name>:<gpu_devices>
```

Where:
- `pod_index`: Pod index (0, 1, 2, ...)
- `node_name`: Target Kubernetes node name
- `gpu_devices`: Comma-separated GPU device IDs for CUDA_VISIBLE_DEVICES

### Example Configurations

**Single GPU per Pod:**
```yaml
gpu-scheduling-map: |
  0=node1:0
  1=node1:1
  2=node2:0
```

**Multiple GPUs per Pod:**
```yaml
gpu-scheduling-map: |
  0=node1:0,1
  1=node2:2,3
  2=node3:0,1,2,3
```

## Repository Structure

```
gpu-scheduler/
â”œâ”€â”€ .gitlab-ci.yml                    # CI/CD pipeline configuration
â”œâ”€â”€ README.md                         # Main documentation
â”œâ”€â”€ COMPLETE_SETUP_GUIDE.md           # Step-by-step setup guide
â”œâ”€â”€ gpu-scheduler/                    # Main scheduler service
â”‚   â”œâ”€â”€ Dockerfile                    # Container image build
â”‚   â”œâ”€â”€ scheduler.py                  # Custom scheduler logic
â”‚   â”œâ”€â”€ webhook_server.py             # Admission webhook server
â”‚   â”œâ”€â”€ health_server.py              # Health check endpoints
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚   â”œâ”€â”€ generate-webhook-certs.sh     # TLS certificate generation
â”‚   â”œâ”€â”€ test_basic.py                 # Unit tests
â”‚   â””â”€â”€ README.md                     # Service documentation
â”œâ”€â”€ gpu-scheduler-check/              # Test/validation service
â”‚   â”œâ”€â”€ Dockerfile                    # Container image build
â”‚   â”œâ”€â”€ main.py                       # Test service logic
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚   â””â”€â”€ README.md                     # Service documentation
â”œâ”€â”€ charts/                           # Helm deployment charts
â”‚   â”œâ”€â”€ gpu-scheduler/                # Main scheduler chart
â”‚   â”‚   â”œâ”€â”€ Chart.yaml                # Chart metadata
â”‚   â”‚   â”œâ”€â”€ values.yaml               # Default configuration
â”‚   â”‚   â”œâ”€â”€ templates/                # Kubernetes manifests
â”‚   â”‚   â””â”€â”€ README.md                 # Chart documentation
â”‚   â””â”€â”€ gpu-scheduler-check/          # Test service chart
â”‚       â”œâ”€â”€ Chart.yaml                # Chart metadata
â”‚       â”œâ”€â”€ values.yaml               # Default configuration
â”‚       â”œâ”€â”€ templates/                # Kubernetes manifests
â”‚       â””â”€â”€ README.md                 # Chart documentation
â”œâ”€â”€ argocd/                           # GitOps deployment configs
â”‚   â”œâ”€â”€ gpu-scheduler-project.yaml           # ArgoCD project definition
â”‚   â”œâ”€â”€ gpu-scheduler-complete-applicationset.yaml  # Complete deployment
â”‚   â”œâ”€â”€ local-cluster-secret.yaml            # Local cluster registration
â”‚   â”œâ”€â”€ validate-applicationset.sh           # Validation script
â”‚   â””â”€â”€ README.md                            # GitOps documentation
â”œâ”€â”€ cluster/                          # Local development setup
â”‚   â”œâ”€â”€ kind-config.yaml              # KinD cluster configuration
â”‚   â””â”€â”€ setup-cluster.sh              # Cluster setup script
â””â”€â”€ outputs/                          # Test results and logs
```

### Test Service Configuration

Configure test service behavior:

```yaml
# values.yaml
replicaCount: 5

gpuScheduling:
  enabled: true
  schedulerName: gpu-scheduler
  schedulingMap:
    - podIndex: 0
      nodeName: node1
      gpuDevices: "0,1"
    - podIndex: 1
      nodeName: node2
      gpuDevices: "2"
    - podIndex: 2
      nodeName: node3
      gpuDevices: "0,1,2"
    - podIndex: 3
      nodeName: node4
      gpuDevices: "3"
    - podIndex: 4
      nodeName: node4
      gpuDevices: "3"

testService:
  logInterval: 10
```

## Monitoring

### Health Checks

The scheduler provides health endpoints:

```bash
# Check scheduler health
kubectl port-forward svc/gpu-scheduler 8080:8080 -n gpu-scheduler-system
curl http://localhost:8080/health

# Check webhook health
kubectl logs -l app.kubernetes.io/name=gpu-scheduler -n gpu-scheduler-system -c webhook
```

### Validation

Monitor test service logs to verify GPU assignments:

```bash
# View test logs
kubectl logs -l app.kubernetes.io/name=gpu-scheduler-check -f

# Expected output:
# Node: node1, CUDA_VISIBLE_DEVICES: 0,1
# Node: node2, CUDA_VISIBLE_DEVICES: 2
# Node: node3, CUDA_VISIBLE_DEVICES: 0,1,2
# Node: node4, CUDA_VISIBLE_DEVICES: 3
# Node: node4, CUDA_VISIBLE_DEVICES: 3
```

## Documentation

### ğŸ“š **Getting Started**

**Option 1: Automated Setup with Ansible (Recommended)**
```bash
# Install Ansible and dependencies
pip3 install ansible
cd ansible-gpu-scheduler
ansible-galaxy collection install -r requirements.yml

# Run complete automated setup
ansible-playbook -i inventory.yml playbook.yml
```

**Option 2: Manual Setup**
- **[Complete Setup Guide](COMPLETE_SETUP_GUIDE.md)** - Step-by-step guide from zero to working system

The Ansible playbook automates the entire setup process including:
- Prerequisites verification (Docker, kubectl, Helm, KinD)
- KinD cluster creation with GPU node labeling
- Container image building and deployment
- GPU scheduler and webhook deployment
- Test service validation
- ArgoCD GitOps configuration


### Development Guidelines

- Follow Python PEP 8 style guide
- Add unit tests for new features
- Update documentation for API changes
- Test changes with local KinD cluster
- Ensure Helm charts lint successfully

## Security

- All containers run as non-root users
- RBAC follows principle of least privilege
- Security contexts enforce read-only filesystems where possible
- Regular security scanning via CI/CD pipeline